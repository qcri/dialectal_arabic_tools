# -*- coding: utf8 -*-
import argparse
import codecs
from collections import Counter
from itertools import chain
import numpy as np
from keras.models import load_model
from keras.preprocessing import sequence
from keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, ChainCRF, Dense, TimeDistributed
import pkg_resources

# http://setuptools.readthedocs.io/en/latest/pkg_resources.html#resourcemanager-api
keras_model = pkg_resources.resource_filename(__name__, "files/models/segmenter.hdf5")
# input_file = pkg_resources.resource_filename(__name__, "files/example.in.txt")
# output_file = pkg_resources.resource_filename(__name__, "files/example.out.txt")


__author__ = 'disooqi'
__created__ = '21 Sep 2017'

parser = argparse.ArgumentParser(description="Segmentation module for the Dialectal Arabic")
group = parser.add_mutually_exclusive_group()
# file related options
parser.add_argument("-d", "--files-dir", help="directory containing train, test and dev file [default: %(default)s]")
parser.add_argument("-g", "--log-file", dest="log_file", help="log file [default: %(default)s]")
parser.add_argument("-p", "--model-file", dest="model_file",
                    help="directory to save the best models [default: %(default)s]")
parser.add_argument("-r", "--train-set", dest="train_set",
                    help="maximul sentence length (for fixed size input) [default: %(default)s]")  # input size
parser.add_argument("-v", "--dev-set", dest="validation_set",
                    help="source vocabulary size [default: %(default)s]")  # emb matrix row size
parser.add_argument("-s", "--test-set", dest="test_set",
                    help="target vocabulary size [default: %(default)s]")  # emb matrix row size
parser.add_argument('-i', '--input-file', dest='input_file', help='text file to be segmented [default: %(default)s]')
parser.add_argument('-o', '--output-file', dest='output_file',
                    help='the file used to save the result of segmentation [default: %(default)s]')
parser.add_argument('-f', '--format', help='format of the output-file [default: %(default)s]')
# network related
# input
parser.add_argument("-t", "--max-length", dest="maxlen", type=int,
                    help="maximul sentence length (for fixed size input) [default: %(default)s]")  # input size
parser.add_argument("-e", "--emb-size", dest="word_embedding_dim", type=int,
                    help="dimension of embedding [default: %(default)s]")  # emb matrix col size
# learning related
parser.add_argument("-a", "--learning-algorithm", dest="learn_alg",
                    help="optimization algorithm (adam, sgd, adagrad, rmsprop, adadelta) [default: %(default)s]")
parser.add_argument("-b", "--minibatch-size", dest="batch_size", type=int, help="minibatch size [default: %(default)s]")
parser.add_argument("-n", "--epochs", dest="nb_epoch", type=int, help="nb of epochs [default: %(default)s]")
# others
parser.add_argument("-V", "--verbose-level", dest="verbose_level",
                    help="verbosity level (0 < 1 < 2) [default: %(default)s]")
parser.add_argument('-m', '--mode',
                    help='choose between training or testing mode (segmentation) [default: %(default)s]')

parser.set_defaults(
    # file
    data_dir="files/",
    model_file=keras_model,
    # train_set="./files/all_train_f01.txt",
    # validation_set="./files/all_dev_f01.txt",
    # test_set="./files/all_test_f01.txt",
    log_file="run.log",
    # input_file=input_file,
    # output_file=output_file,
    format='conll',
    # network related
    maxlen=50,  # cut texts after this number of words (among top max_features most common words)
    word_embedding_dim=200,

    # learning related
    learn_alg="adam",  # sgd, adagrad, rmsprop, adadelta, adam (default)
    # loss = "binary_crossentropy" # hinge, squared_hinge, binary_crossentropy (default)
    batch_size=10,
    nb_epoch=100,
    verbose_level=1,
    lstm_dim=100,
    max_features=217,  # len(index2word)   and   len(_invert_index(index2word))  ## For evaluation mode only
    nb_pos_tags=4,  # len(index2pos)                                             ## For evaluation mode only
)

args = parser.parse_args()

index2pos = ['S', 'B', 'E', 'M']
pos2index = {'S': 0, 'B': 1, 'E': 2, 'M': 3}
# For evaluation mode only
index2word = ['<PAD>', '<UNK>', 'ÿß', 'ŸÑ', 'Ÿä', 'Ÿà', 'ŸÜ', 'ŸÖ', 'ÿ®', 'ÿ™', 'Ÿá', 'ÿ±', 'ŸÉ', 'ÿπ', 'ÿØ', 'ÿ≠', 'ŸÅ', 'ÿ≥', 'ÿ¥', 'ŸÇ', 'ÿ©', 'ÿ¨', 'ÿ£', '.', 'ÿµ', 'ÿÆ', 'ÿ∑', 'ÿ≤', 'Ÿâ', 'ÿ∫', 'a', 'ÿ∂', '?', '!', 'ÿ•', 'ÿ∞', '_', '@', '#', 't', 'e', ':', 'o', 'ÿå', 'h', 'i', 'ÿ´', 'ÿü', '/', 'l', 'r', 'm', 's', 'n', 'ÿ∏', 'ÿ¶', 'ÿ°', 'ÿ¢', 'd', 'y', 'p', '"', 'c', '1', 'b', '0', ')', '-', 'A', 'S', '2', 'u', 'w', '(', ',', '9', 'k', 'D', 'M', 'f', '5', 'ÿ§', 'z', '7', 'g', '3', 'T', 'H', '8', '4', 'j', 'R', '€Å', 'N', 'B', '6', 'v', 'J', 'L', 'E', '*', '^', 'G', 'Y', 'K', 'F', '~', "'", 'Q', 'V', 'I', 'C', 'W', '⁄Ø', 'O', 'P', '⁄Ü', 'Ôªª', 'q', 'Z', '>', 'x', 'ÿõ', 'U', '⁄æ', '‚Äπ', 'Ÿ§', '‚Ä¶', 'üòÇ', '‚Äù', '‚Ä∫', 'Ô∫ç', 'Ôª¥', 'Ô∫é', '=', 'üò≠', '⁄©', 'X', 'Ôªõ', 'Ôªß', '<', '[', '⁄§', 'Ô∫Æ', 'Ô∫¥', 'Ÿß', 'üòä', 'üòù', 'üòò', 'üòÖ', '|', 'Ôª∑', 'üôä', '‚Äú', 'Ÿ¢', '\\', 'Ôª¶', 'Ÿ°', 'Ô∫ò', 'Ô∫™', 'Ôªü', 'Ôªã', 'Ô∫∏', 'Ô∫ê', 'Ô∫£', 'üçâ', 'Ÿ£', 'üòç', '‚ò∫', '‚ù§', 'Ô∏è', 'üòú', '‚Ä¢', ']', '¬ª', ';', 'Ôªπ', 'Ô∫≥', 'Ôª¨', 'Ÿ≤', 'Ôª£', 'Ôªå', 'Ô∫ë', 'Ôªä', 'Ôªö', 'ÔªÉ', 'Ôª®', 'Ôª§', 'Ôª≠', 'Ô∫°', 'Ô∫Ñ', 'Ô∫ü', 'ÔªÆ', 'Ô∫Ø', 'Ôªû', 'Ôª•', 'Ÿ†', 'Ÿ®', 'üòã', 'üåπ', 'Ÿì', 'üò•', 'Ÿ´', 'Ÿ•', '‚òπ', 'Ÿ¶', 'üôà', 'Ÿ©', '\ue756', 'üòå', 'üòî', '‚ô•', 'üëç', 'üòÜ', 'üòö', '$', '%']

# For evaluation mode only
word2index = {'<PAD>': 0, '<UNK>': 1, 'ÿß': 2, 'ŸÑ': 3, 'Ÿä': 4, 'Ÿà': 5, 'ŸÜ': 6, 'ŸÖ': 7, 'ÿ®': 8, 'ÿ™': 9, 'Ÿá': 10, 'ÿ±': 11, 'ŸÉ': 12, 'ÿπ': 13, 'ÿØ': 14, 'ÿ≠': 15, 'ŸÅ': 16, 'ÿ≥': 17, 'ÿ¥': 18, 'ŸÇ': 19, 'ÿ©': 20, 'ÿ¨': 21, 'ÿ£': 22, '.': 23, 'ÿµ': 24, 'ÿÆ': 25, 'ÿ∑': 26, 'ÿ≤': 27, 'Ÿâ': 28, 'ÿ∫': 29, 'a': 30, 'ÿ∂': 31, '?': 32, '!': 33, 'ÿ•': 34, 'ÿ∞': 35, '_': 36, '@': 37, '#': 38, 't': 39, 'e': 40, ':': 41, 'o': 42, 'ÿå': 43, 'h': 44, 'i': 45, 'ÿ´': 46, 'ÿü': 47, '/': 48, 'l': 49, 'r': 50, 'm': 51, 's': 52, 'n': 53, 'ÿ∏': 54, 'ÿ¶': 55, 'ÿ°': 56, 'ÿ¢': 57, 'd': 58, 'y': 59, 'p': 60, '"': 61, 'c': 62, '1': 63, 'b': 64, '0': 65, ')': 66, '-': 67, 'A': 68, 'S': 69, '2': 70, 'u': 71, 'w': 72, '(': 73, ',': 74, '9': 75, 'k': 76, 'D': 77, 'M': 78, 'f': 79, '5': 80, 'ÿ§': 81, 'z': 82, '7': 83, 'g': 84, '3': 85, 'T': 86, 'H': 87, '8': 88, '4': 89, 'j': 90, 'R': 91, '€Å': 92, 'N': 93, 'B': 94, '6': 95, 'v': 96, 'J': 97, 'L': 98, 'E': 99, '*': 100, '^': 101, 'G': 102, 'Y': 103, 'K': 104, 'F': 105, '~': 106, "'": 107, 'Q': 108, 'V': 109, 'I': 110, 'C': 111, 'W': 112, '⁄Ø': 113, 'O': 114, 'P': 115, '⁄Ü': 116, 'Ôªª': 117, 'q': 118, 'Z': 119, '>': 120, 'x': 121, 'ÿõ': 122, 'U': 123, '⁄æ': 124, '‚Äπ': 125, 'Ÿ§': 126, '‚Ä¶': 127, 'üòÇ': 128, '‚Äù': 129, '‚Ä∫': 130, 'Ô∫ç': 131, 'Ôª¥': 132, 'Ô∫é': 133, '=': 134, 'üò≠': 135, '⁄©': 136, 'X': 137, 'Ôªõ': 138, 'Ôªß': 139, '<': 140, '[': 141, '⁄§': 142, 'Ô∫Æ': 143, 'Ô∫¥': 144, 'Ÿß': 145, 'üòä': 146, 'üòù': 147, 'üòò': 148, 'üòÖ': 149, '|': 150, 'Ôª∑': 151, 'üôä': 152, '‚Äú': 153, 'Ÿ¢': 154, '\\': 155, 'Ôª¶': 156, 'Ÿ°': 157, 'Ô∫ò': 158, 'Ô∫™': 159, 'Ôªü': 160, 'Ôªã': 161, 'Ô∫∏': 162, 'Ô∫ê': 163, 'Ô∫£': 164, 'üçâ': 165, 'Ÿ£': 166, 'üòç': 167, '‚ò∫': 168, '‚ù§': 169, 'Ô∏è': 170, 'üòú': 171, '‚Ä¢': 172, ']': 173, '¬ª': 174, ';': 175, 'Ôªπ': 176, 'Ô∫≥': 177, 'Ôª¨': 178, 'Ÿ≤': 179, 'Ôª£': 180, 'Ôªå': 181, 'Ô∫ë': 182, 'Ôªä': 183, 'Ôªö': 184, 'ÔªÉ': 185, 'Ôª®': 186, 'Ôª§': 187, 'Ôª≠': 188, 'Ô∫°': 189, 'Ô∫Ñ': 190, 'Ô∫ü': 191, 'ÔªÆ': 192, 'Ô∫Ø': 193, 'Ôªû': 194, 'Ôª•': 195, 'Ÿ†': 196, 'Ÿ®': 197, 'üòã': 198, 'üåπ': 199, 'Ÿì': 200, 'üò•': 201, 'Ÿ´': 202, 'Ÿ•': 203, '‚òπ': 204, 'Ÿ¶': 205, 'üôà': 206, 'Ÿ©': 207, '\ue756': 208, 'üòå': 209, 'üòî': 210, '‚ô•': 211, 'üëç': 212, 'üòÜ': 213, 'üòö': 214, '$': 215, '%': 216}

def remove_diacrtics(token):
    # TODO implement 'remove_diacrtics'
    return token

def _fit_term_index(terms, reserved=[], preprocess=lambda x: x):
    all_terms = chain(*terms)
    all_terms = map(preprocess, all_terms)
    term_freqs = Counter(all_terms).most_common()
    id2term = reserved + [term for term, tf in term_freqs]
    return id2term


def _invert_index(id2term):
    return {term: i for i, term in enumerate(id2term)}


def load_file_as_words(file_path):
    '''
    :param file_path: path of a fule contains a conll format of segmentation
    :return: a tuple of two lists of lists of characters and tags
    '''
    with codecs.open(file_path, encoding='utf-8') as conell:
        list_of_lines = [line.strip().split() for line in conell if len(line.strip().split()) == 2]
        chars, targets = list(zip(*list_of_lines))
        words = ''.join(chars).split('WB')
        target_of_words = ''.join(targets).split('WB')

        words = [tuple(word) for word in words]
        target_of_words = [tuple(trg) for trg in target_of_words]

        return words, target_of_words

def load_txt_as_ch_list(file_path):
    words = list()
    with codecs.open(file_path, encoding='utf-8') as plan_txt:
        for line in plan_txt:
            tokens = line.strip().split()
            for token in tokens:
                words.append(list(token))

    return words


def build_embeddings(mxl):
    # Add UNKNOWN and
    targets = np.array([1 for i in range(0, mxl)])
    one_hot_targets = np.eye(args.word_embedding_dim)[targets]
    return one_hot_targets


def segment_text(text, dl_model=keras_model):
    X_chars = list()
    tokens = text.strip().split()
    for token in tokens:
        X_chars.append(list(token))

    prediction = __segment__(X_chars, dl_model)

    segmentation = list()
    for pred, word in zip(np.argmax(prediction, axis=2), X_chars):
        assert len(pred) >= len(word)

        for ch, est in zip(word, pred):
            segmentation.append(ch)
            if index2pos[est] in ['S', 'E']:
                segmentation.append('+')
        else:
            segmentation[-1] = ' '

    return ''.join(segmentation)


def segment_file(infile, outfile, dl_model=keras_model):
    X_chars = load_txt_as_ch_list(infile)
    prediction = __segment__(X_chars, dl_model)

    with codecs.open(outfile, mode='w', encoding='utf-8') as results:
        for pred, word in zip(np.argmax(prediction, axis=2), X_chars):
            assert len(pred) >= len(word)

            for ch, est in zip(word, pred):
                results.write(ch + '\t' + index2pos[est] + '\n')
            else:
                results.write('WB\tWB\n')


def __segment__(X_chars, dl_model=keras_model):
    embeddings = build_embeddings(args.max_features)
    X_idxs = np.array([[word2index.get(w, word2index['<UNK>']) for w in words] for words in X_chars])
    X_idxs_padded = sequence.pad_sequences(X_idxs, maxlen=args.maxlen, padding='post')

    word_input = Input(shape=(args.maxlen,), dtype='int32', name='word_input')
    word_emb = Embedding(embeddings.shape[0], args.word_embedding_dim, input_length=args.maxlen, name='word_emb',
                         weights=[embeddings])(word_input)
    word_emb_d = Dropout(0.5)(word_emb)
    bilstm = Bidirectional(LSTM(args.lstm_dim, return_sequences=True))(word_emb_d)
    bilstm_d = Dropout(0.5)(bilstm)
    dense = TimeDistributed(Dense(args.nb_pos_tags))(bilstm_d)
    crf = ChainCRF()
    crf_output = crf(dense)
    model = load_model(dl_model,
                       custom_objects={'ChainCRF': ChainCRF, 'sparse_loss': crf.sparse_loss}, compile=False)
    model.compile(loss=crf.sparse_loss, optimizer='adam', metrics=['sparse_categorical_accuracy'])

    return model.predict(X_idxs_padded, args.batch_size, verbose=0)




if '__name__' == '__main__':
    pass

